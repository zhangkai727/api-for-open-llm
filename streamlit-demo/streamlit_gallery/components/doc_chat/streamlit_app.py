import os
from pathlib import Path

import pandas as pd
import streamlit as st
from langchain_openai import OpenAIEmbeddings
from openai import OpenAI

from .utils import DocServer, DOCQA_PROMPT


def main():
    # è®¾ç½®ä¸Šä¼ æ–‡ä»¶å¤¹è·¯å¾„ï¼Œä½äºè„šæœ¬æ–‡ä»¶çš„ä¸Šä¸‰çº§ç›®å½•ä¸­çš„ "upload_files" æ–‡ä»¶å¤¹
    UPLOAD_FOLDER = os.path.join(Path(__file__).parents[3], "upload_files")

    # åˆ›å»ºä¸Šä¼ æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(UPLOAD_FOLDER, exist_ok=True)

    # ç¼“å­˜åŠ è½½æ–‡æ¡£æœåŠ¡å™¨
    @st.cache_resource
    def load_doc_server():
        # åˆå§‹åŒ– OpenAI Embeddings
        embeddings = OpenAIEmbeddings(
            openai_api_base=os.getenv("EMBEDDING_API_BASE"),
            openai_api_key=os.getenv("API_KEY", ""),
        )
        # åˆ›å»ºæ–‡æ¡£æœåŠ¡å™¨å®ä¾‹
        server = DocServer(embeddings)
        return server

    # åŠ è½½æ–‡æ¡£æœåŠ¡å™¨
    server = load_doc_server()

    # ç¼“å­˜åˆ›å»ºæ–‡ä»¶ç´¢å¼•
    @st.cache_resource
    def create_file_index(file, chunk_size, chunk_overlap, table_name):
        # è·å–æ–‡ä»¶åå¹¶è®¾ç½®æ–‡ä»¶è·¯å¾„
        filename = file.name
        filepath = f"{UPLOAD_FOLDER}/{filename}"
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        with open(filepath, "wb") as f:
            f.write(file.read())

        # ä¸Šä¼ æ–‡ä»¶å¹¶åˆ›å»ºç´¢å¼•ï¼Œè·å–æ–‡ä»¶ID
        file_id = server.upload(
            filepath,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            table_name=table_name,
        )
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        st.session_state.update(dict(file_id=file_id))

        # åˆ é™¤æœ¬åœ°ä¿å­˜çš„æ–‡ä»¶
        os.remove(filepath)
        return file.name

    # ç¼“å­˜åˆ›å»ºURLç´¢å¼•
    @st.cache_resource
    def create_url_index(url, chunk_size, chunk_overlap, table_name):
        # ä¸Šä¼ URLå¹¶åˆ›å»ºç´¢å¼•ï¼Œè·å–è¡¨å
        table_name = server.upload(
            url=url,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            table_name=table_name,
        )
        return table_name

    # åˆ é™¤ç´¢å¼•
    def delete_index(table_name):
        # åˆ é™¤æŒ‡å®šè¡¨åçš„ç´¢å¼•
        server.delete(table_name)
        return table_name

    # è®¾ç½®æ ‡é¢˜
    st.title("ğŸ’¬ Document Chatbot")

    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    client = OpenAI(
        api_key=st.session_state.get("api_key", "xxx"),
        base_url=st.session_state.get("base_url", "xxx"),
    )

    # è®¾ç½®é¡µé¢å¸ƒå±€
    col1, col2, col3 = st.columns([3, 3, 4])

    with col1:
        with st.expander(label="âœ¨ ç®€ä»‹"):
            # æ˜¾ç¤ºç®€ä»‹
            st.markdown("""+ æ–‡æ¡£é—®ç­”æ˜¯æŒ‡ä»æ–‡æœ¬æˆ–æ–‡æ¡£ä¸­æ£€ç´¢å’Œç†è§£ç›¸å…³ä¿¡æ¯ï¼Œç„¶åå›ç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜ã€‚

+ è¯¥æŠ€æœ¯é€šå¸¸ç”¨äºä¿¡æ¯æ£€ç´¢ã€çŸ¥è¯†å›¾è°±é—®ç­”ã€æ™ºèƒ½å®¢æœç­‰é¢†åŸŸã€‚

+ æœ¬é¡¹ç›®æ”¯æŒ**æ–‡æ¡£é—®ç­”**å’Œ**URLé—®ç­”**""")
            # é€‰æ‹©ä¸Šä¼ æ–‡ä»¶ç±»å‹
            mode = st.selectbox("è¯·é€‰æ‹©ä¸Šä¼ æ–‡ä»¶ç±»å‹", options=["æ–‡ä»¶", "ç½‘å€"])
            # é€‰æ‹©æ˜¯å¦é‡æ’åº
            rerank = st.checkbox("ğŸš€ é‡æ’åº")

    with col2:
        with st.expander("ğŸ“– çŸ¥è¯†åº“åˆ—è¡¨", False):
            # æ˜¾ç¤ºçŸ¥è¯†åº“åˆ—è¡¨
            vector_store_names = server.db.table_names()
            st.dataframe(pd.DataFrame({"vector_store_name": vector_store_names}))

    with col3:
        with st.expander("ğŸ“šâ€ é…ç½®"):
            # è¾“å…¥ç½‘å€
            url = st.text_input("ç½‘å€", placeholder="https://qwenlm.github.io/zh/blog/codeqwen1.5/")
            # ä¸Šä¼ æ–‡ä»¶
            file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", accept_multiple_files=False)

            # é€‰æ‹©æˆ–åˆ›å»ºçŸ¥è¯†åº“
            table_name = st.text_input(
                "é€‰æ‹©æˆ–è€…åˆ›å»ºçŸ¥è¯†åº“",
                placeholder=vector_store_names[0] if vector_store_names else "test"
            )

            col5, col6 = st.columns([5, 5])
            with col5:
                create = st.button("âœ… å¯¼å…¥çŸ¥è¯†åº“")
            with col6:
                if st.button("âŒ åˆ é™¤çŸ¥è¯†åº“"):
                    _ = delete_index(table_name)

            # æ ¹æ®æ–‡ä»¶å’Œæ¨¡å¼åˆ›å»ºæ–‡ä»¶ç´¢å¼•
            if file and mode == "æ–‡ä»¶" and table_name and create:
                create_file_index(
                    file,
                    chunk_size=st.session_state.get("chunk_size", 250),
                    chunk_overlap=st.session_state.get("chunk_overlap", 50),
                    table_name=table_name,
                )

            # æ ¹æ®URLå’Œæ¨¡å¼åˆ›å»ºURLç´¢å¼•
            if url and mode == "ç½‘å€" and table_name and create:
                create_url_index(
                    url,
                    chunk_size=st.session_state.get("chunk_size", 250),
                    chunk_overlap=st.session_state.get("chunk_overlap", 50),
                    table_name=table_name,
                )

            # æ›´æ–°ä¼šè¯çŠ¶æ€
            st.session_state.update(dict(table_name=table_name))

    # åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºä¼šè¯ä¸­çš„æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
        if message["role"] == "assistant" and isinstance(message["reference"], pd.DataFrame):
            with st.expander(label="å±•ç¤ºæœç´¢ç»“æœ"):
                st.dataframe(message["reference"], use_container_width=True)

    # å¤„ç†ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
    if prompt := st.chat_input("What is up?"):
        table_name = st.session_state.get("table_name", None)
        doc_prompt, reference = None, None
        if table_name is not None:
            # åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯
            result = server.search(
                query=prompt,
                top_k=st.session_state.get("top_k", 3),
                table_name=table_name,
                rerank=rerank,
            )

            context = "\n\n".join(doc for doc in result["text"].tolist())
            doc_prompt = DOCQA_PROMPT.format(query=prompt, context=context)
            reference = result

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            pyload = dict(
                model=st.session_state.get("model_name", "xxx"),
                messages=[
                    {
                        "role": m["role"],
                        "content": m["content"]
                    }
                    for m in st.session_state.messages[:-1]
                ] + [
                        {
                            "role": "user",
                            "content": doc_prompt or prompt
                        }
                ],
                stream=True,
                max_tokens=st.session_state.get("max_tokens", 512),
                temperature=st.session_state.get("temperature", 0.9),
            )

            # ç”Ÿæˆæ¨¡å‹å›å¤
            for response in client.chat.completions.create(**pyload):
                full_response += response.choices[0].delta.content or ""
                message_placeholder.markdown(full_response + "â–Œ")

            message_placeholder.markdown(full_response)
            if isinstance(reference, pd.DataFrame):
                with st.expander(label="å±•ç¤ºæœç´¢ç»“æœ"):
                    st.dataframe(reference, use_container_width=True)

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        st.session_state.messages.append(
            {
                "role": "assistant",
                "content": full_response,
                "reference": reference,
            }
        )


if __name__ == "__main__":
    main()  # è¿è¡Œä¸»å‡½æ•°


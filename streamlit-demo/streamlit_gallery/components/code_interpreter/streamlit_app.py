import os

import streamlit as st
from openai import OpenAI

from .utils import CodeKernel, extract_code, execute, postprocess_text


@st.cache_resource  # ä½¿ç”¨Streamlitçš„ç¼“å­˜è£…é¥°å™¨ç¼“å­˜èµ„æº
def get_kernel():  # å®šä¹‰è·å–ä»£ç å†…æ ¸çš„å‡½æ•°
    return CodeKernel()  # è¿”å›ä¸€ä¸ªCodeKernelå®ä¾‹


SYSTEM_MESSAGE = [  # å®šä¹‰ç³»ç»Ÿæ¶ˆæ¯
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä½æ™ºèƒ½AIåŠ©æ‰‹ï¼Œä½ å«ChatGLMï¼Œä½ è¿æ¥ç€ä¸€å°ç”µè„‘ï¼Œä½†è¯·æ³¨æ„ä¸èƒ½è”ç½‘ã€‚åœ¨ä½¿ç”¨Pythonè§£å†³ä»»åŠ¡æ—¶ï¼Œä½ å¯ä»¥è¿è¡Œä»£ç å¹¶å¾—åˆ°ç»“æœï¼Œå¦‚æœè¿è¡Œç»“æœæœ‰é”™è¯¯ï¼Œä½ éœ€è¦å°½å¯èƒ½å¯¹ä»£ç è¿›è¡Œæ”¹è¿›ã€‚ä½ å¯ä»¥å¤„ç†ç”¨æˆ·ä¸Šä¼ åˆ°ç”µè„‘ä¸Šçš„æ–‡ä»¶ï¼Œæ–‡ä»¶é»˜è®¤å­˜å‚¨è·¯å¾„æ˜¯/mnt/data/ã€‚"
    }
]


def chat_once(message_placeholder, client: OpenAI):  # å®šä¹‰ä¸€æ¬¡èŠå¤©çš„å‡½æ•°
    params = dict(  # è®¾ç½®èŠå¤©å‚æ•°
        model="chatglm3",  # ä½¿ç”¨çš„æ¨¡å‹
        messages=SYSTEM_MESSAGE + st.session_state.messages,  # ç³»ç»Ÿæ¶ˆæ¯å’Œä¼šè¯æ¶ˆæ¯
        stream=True,  # å¯ç”¨æµå¼å“åº”
        max_tokens=st.session_state.get("max_tokens", 512),  # æœ€å¤§ä»¤ç‰Œæ•°
        temperature=st.session_state.get("temperature", 0.9),  # æ¸©åº¦å‚æ•°
    )
    response = client.chat.completions.create(**params)  # è°ƒç”¨OpenAI APIç”ŸæˆèŠå¤©å›å¤

    display = ""  # åˆå§‹åŒ–æ˜¾ç¤ºå­—ç¬¦ä¸²
    for _ in range(5):  # é‡è¯•5æ¬¡
        full_response = ""  # åˆå§‹åŒ–å®Œæ•´å“åº”å­—ç¬¦ä¸²
        for chunk in response:  # éå†å“åº”æµ
            content = chunk.choices[0].delta.content or ""  # è·å–å“åº”å†…å®¹
            full_response += content  # æ‹¼æ¥å®Œæ•´å“åº”
            display += content  # æ‹¼æ¥æ˜¾ç¤ºå†…å®¹
            message_placeholder.markdown(postprocess_text(display) + "â–Œ")  # æ˜¾ç¤ºå½“å‰ç”Ÿæˆçš„éƒ¨åˆ†å“åº”

            if chunk.choices[0].finish_reason == "stop":  # å¦‚æœå“åº”å®Œæˆ
                message_placeholder.markdown(postprocess_text(display) + "â–Œ")  # æ˜¾ç¤ºå®Œæ•´å“åº”
                st.session_state.messages.append(  # å°†åŠ©æ‰‹çš„å®Œæ•´å“åº”æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­
                    {
                        "role": "assistant",
                        "content": full_response
                    }
                )
                return  # è¿”å›

            elif chunk.choices[0].finish_reason == "function_call":  # å¦‚æœéœ€è¦è°ƒç”¨å‡½æ•°
                try:
                    code = extract_code(full_response)  # æå–ä»£ç 
                except:
                    continue  # ç»§ç»­ä¸‹ä¸€ä¸ªå“åº”

                with message_placeholder:  # æ˜¾ç¤ºä»£ç æ‰§è¡ŒçŠ¶æ€
                    with st.spinner("Executing code..."):  # æ˜¾ç¤ºæ‰§è¡Œä»£ç çš„åŠ è½½çŠ¶æ€
                        try:
                            res_type, res = execute(code, get_kernel())  # æ‰§è¡Œä»£ç å¹¶è·å–ç»“æœ
                        except Exception as e:
                            st.error(f"Error when executing code: {e}")  # æ˜¾ç¤ºæ‰§è¡Œä»£ç é”™è¯¯
                            return  # è¿”å›

                if res_type == "text":  # å¦‚æœç»“æœæ˜¯æ–‡æœ¬
                    res = postprocess_text(res)  # åå¤„ç†æ–‡æœ¬
                    display += "\n" + res  # æ‹¼æ¥æ˜¾ç¤ºå†…å®¹
                    message_placeholder.markdown(postprocess_text(display) + "â–Œ")  # æ˜¾ç¤ºå½“å‰ç”Ÿæˆçš„éƒ¨åˆ†å“åº”
                elif res_type == "image":  # å¦‚æœç»“æœæ˜¯å›¾åƒ
                    st.image(res)  # æ˜¾ç¤ºå›¾åƒ

                st.session_state.messages.append(  # å°†åŠ©æ‰‹çš„å®Œæ•´å“åº”æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­
                    {
                        "role": "assistant",
                        "content": full_response,
                    }
                )
                st.session_state.messages.append(  # å°†å‡½æ•°è°ƒç”¨ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­
                    {
                        "role": "function",
                        "name": "interpreter",
                        "content": "[Image]" if res_type == "image" else res,  # è°ƒç”¨å‡½æ•°è¿”å›ç»“æœ
                    }
                )

                break  # ç»“æŸå½“å‰å“åº”å¤„ç†

        params["messages"] = st.session_state.messages  # æ›´æ–°æ¶ˆæ¯ä¸Šä¸‹æ–‡
        response = client.chat.completions.create(**params)  # é‡æ–°è°ƒç”¨OpenAI APIç”ŸæˆèŠå¤©å›å¤


def main():  # å®šä¹‰ä¸»å‡½æ•°
    st.title("ğŸ’¬ Code Interpreter")  # è®¾ç½®é¡µé¢æ ‡é¢˜ä¸º"ğŸ’¬ Code Interpreter"

    client = OpenAI(  # åˆ›å»ºOpenAIå®¢æˆ·ç«¯å®ä¾‹
        api_key=os.getenv("API_KEY"),  # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        base_url=os.getenv("INTERPRETER_CHAT_API_BASE"),  # ä»ç¯å¢ƒå˜é‡è·å–åŸºç¡€URL
    )

    if "messages" not in st.session_state:  # å¦‚æœä¼šè¯çŠ¶æ€ä¸­æ²¡æœ‰æ¶ˆæ¯åˆ—è¡¨
        st.session_state.messages = []  # åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨

    for message in st.session_state.messages:  # éå†æ¶ˆæ¯åˆ—è¡¨ä¸­çš„æ¯æ¡æ¶ˆæ¯
        role = message["role"]  # è·å–æ¶ˆæ¯çš„è§’è‰²
        if role in ["user", "function"]:  # å¦‚æœè§’è‰²æ˜¯ç”¨æˆ·æˆ–å‡½æ•°
            with st.chat_message("user"):  # æ˜¾ç¤ºç”¨æˆ·çš„èŠå¤©æ¶ˆæ¯
                st.markdown(message["content"])  # ä½¿ç”¨Markdownæ˜¾ç¤ºæ¶ˆæ¯å†…å®¹
        else:  # å¦åˆ™ï¼ˆè§’è‰²æ˜¯åŠ©æ‰‹ï¼‰
            with st.chat_message("assistant"):  # æ˜¾ç¤ºåŠ©æ‰‹çš„èŠå¤©æ¶ˆæ¯
                st.markdown(postprocess_text(message["content"]))  # ä½¿ç”¨Markdownæ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯å†…å®¹

    if prompt := st.chat_input("What is up?"):  # å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹
        st.session_state.messages.append({"role": "user", "content": prompt})  # å°†ç”¨æˆ·è¾“å…¥çš„å†…å®¹æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­
        with st.chat_message("user"):  # æ˜¾ç¤ºç”¨æˆ·çš„èŠå¤©æ¶ˆæ¯
            st.markdown(prompt)  # ä½¿ç”¨Markdownæ˜¾ç¤ºç”¨æˆ·è¾“å…¥çš„å†…å®¹

        with st.chat_message("assistant"):  # æ˜¾ç¤ºåŠ©æ‰‹çš„èŠå¤©æ¶ˆæ¯
            message_placeholder = st.empty()  # åˆ›å»ºä¸€ä¸ªç©ºçš„å ä½ç¬¦ç”¨äºæ˜¾ç¤ºåŠ©æ‰‹çš„å“åº”
            chat_once(message_placeholder, client)  # è°ƒç”¨ä¸€æ¬¡èŠå¤©çš„å‡½æ•°


if __name__ == "__main__":  # å¦‚æœè¯¥è„šæœ¬ä½œä¸ºä¸»ç¨‹åºæ‰§è¡Œ
    main()  # è°ƒç”¨ä¸»å‡½æ•°

